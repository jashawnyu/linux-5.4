./kernel/sched/pelt.h:	if (unlikely(cfs_rq->throttle_count))
./kernel/sched/pelt.h:		return cfs_rq->throttled_clock_task - cfs_rq->throttled_clock_task_time;
./kernel/sched/pelt.h:	return rq_clock_pelt(rq_of(cfs_rq)) - cfs_rq->throttled_clock_task_time;
./kernel/sched/debug.c:	SEQ_printf(m, "cfs_rq[%d]:%s\n", cpu, task_group_path(cfs_rq->tg));
./kernel/sched/debug.c:			SPLIT_NS(cfs_rq->exec_clock));
./kernel/sched/debug.c:	if (rb_first_cached(&cfs_rq->tasks_timeline))
./kernel/sched/debug.c:	min_vruntime = cfs_rq->min_vruntime;
./kernel/sched/debug.c:			cfs_rq->nr_spread_over);
./kernel/sched/debug.c:	SEQ_printf(m, "  .%-30s: %d\n", "nr_running", cfs_rq->nr_running);
./kernel/sched/debug.c:	SEQ_printf(m, "  .%-30s: %ld\n", "load", cfs_rq->load.weight);
./kernel/sched/debug.c:	SEQ_printf(m, "  .%-30s: %ld\n", "runnable_weight", cfs_rq->runnable_weight);
./kernel/sched/debug.c:			cfs_rq->avg.load_avg);
./kernel/sched/debug.c:			cfs_rq->avg.runnable_load_avg);
./kernel/sched/debug.c:			cfs_rq->avg.util_avg);
./kernel/sched/debug.c:			cfs_rq->avg.util_est.enqueued);
./kernel/sched/debug.c:			cfs_rq->removed.load_avg);
./kernel/sched/debug.c:			cfs_rq->removed.util_avg);
./kernel/sched/debug.c:			cfs_rq->removed.runnable_sum);
./kernel/sched/debug.c:			cfs_rq->tg_load_avg_contrib);
./kernel/sched/debug.c:			atomic_long_read(&cfs_rq->tg->load_avg));
./kernel/sched/debug.c:			cfs_rq->throttled);
./kernel/sched/debug.c:			cfs_rq->throttle_count);
./kernel/sched/debug.c:	print_cfs_group_stats(m, cpu, cfs_rq->tg);
./kernel/sched/core.c:	struct sched_entity *curr = (&p->se)->cfs_rq->curr;
./kernel/sched/core.c:	 * Prevent race between setting of cfs_rq->runtime_enabled and
./kernel/sched/core.c:		struct rq *rq = cfs_rq->rq;
./kernel/sched/core.c:		cfs_rq->runtime_enabled = runtime_enabled;
./kernel/sched/core.c:		cfs_rq->runtime_remaining = 0;
./kernel/sched/core.c:		if (cfs_rq->throttled)
./kernel/sched/sched.h:  /*虽然自己是unthrottle状态，但是parent cfs_rq是throttle状态，自己也是没办法运行的。所以throttled_clock_task_time统计的是cfs_rq->throttle_count从非零变成0经历的时间总和*/
./kernel/sched/sched.h:	return cfs_rq->rq;
./kernel/sched/fair.c:	if (cfs_rq && task_group_is_autogroup(cfs_rq->tg))
./kernel/sched/fair.c:		autogroup_path(cfs_rq->tg, path, len);
./kernel/sched/fair.c:	else if (cfs_rq && cfs_rq->tg->css.cgroup)
./kernel/sched/fair.c:		cgroup_path(cfs_rq->tg->css.cgroup, path, len);
./kernel/sched/fair.c:	if (cfs_rq->on_list)
./kernel/sched/fair.c:	cfs_rq->on_list = 1;
./kernel/sched/fair.c:	if (cfs_rq->tg->parent &&
./kernel/sched/fair.c:	    cfs_rq->tg->parent->cfs_rq[cpu]->on_list) {
./kernel/sched/fair.c:		list_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,
./kernel/sched/fair.c:			&(cfs_rq->tg->parent->cfs_rq[cpu]->leaf_cfs_rq_list));
./kernel/sched/fair.c:	if (!cfs_rq->tg->parent) {
./kernel/sched/fair.c:		list_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,
./kernel/sched/fair.c:	list_add_rcu(&cfs_rq->leaf_cfs_rq_list, rq->tmp_alone_branch);
./kernel/sched/fair.c:	rq->tmp_alone_branch = &cfs_rq->leaf_cfs_rq_list;
./kernel/sched/fair.c:	if (cfs_rq->on_list) {
./kernel/sched/fair.c:		if (rq->tmp_alone_branch == &cfs_rq->leaf_cfs_rq_list)
./kernel/sched/fair.c:			rq->tmp_alone_branch = cfs_rq->leaf_cfs_rq_list.prev;
./kernel/sched/fair.c:		list_del_rcu(&cfs_rq->leaf_cfs_rq_list);
./kernel/sched/fair.c:		cfs_rq->on_list = 0;
./kernel/sched/fair.c:	struct sched_entity *curr = cfs_rq->curr;
./kernel/sched/fair.c:	struct rb_node *leftmost = rb_first_cached(&cfs_rq->tasks_timeline);
./kernel/sched/fair.c:	u64 vruntime = cfs_rq->min_vruntime;
./kernel/sched/fair.c:	cfs_rq->min_vruntime = max_vruntime(cfs_rq->min_vruntime, vruntime);
./kernel/sched/fair.c:	cfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;
./kernel/sched/fair.c:	struct rb_node **link = &cfs_rq->tasks_timeline.rb_root.rb_node;
./kernel/sched/fair.c:			       &cfs_rq->tasks_timeline, leftmost);
./kernel/sched/fair.c:	rb_erase_cached(&se->run_node, &cfs_rq->tasks_timeline);
./kernel/sched/fair.c:	struct rb_node *left = rb_first_cached(&cfs_rq->tasks_timeline);
./kernel/sched/fair.c:	struct rb_node *last = rb_last(&cfs_rq->tasks_timeline.rb_root);
./kernel/sched/fair.c:	u64 slice = __sched_period(cfs_rq->nr_running + !se->on_rq);
./kernel/sched/fair.c:		load = &cfs_rq->load;
./kernel/sched/fair.c:			lw = cfs_rq->load;
./kernel/sched/fair.c: *   util_avg = cfs_rq->util_avg / (cfs_rq->load_avg + 1) * se.load.weight
./kernel/sched/fair.c: *   util_avg_cap = (cpu_scale - cfs_rq->avg.util_avg) / 2^n
./kernel/sched/fair.c:	long cap = (long)(cpu_scale - cfs_rq->avg.util_avg) / 2;
./kernel/sched/fair.c:		if (cfs_rq->avg.util_avg != 0) {
./kernel/sched/fair.c:			sa->util_avg  = cfs_rq->avg.util_avg * se->load.weight;
./kernel/sched/fair.c:			sa->util_avg /= (cfs_rq->avg.load_avg + 1);
./kernel/sched/fair.c:	struct sched_entity *curr = cfs_rq->curr;
./kernel/sched/fair.c:	schedstat_add(cfs_rq->exec_clock, delta_exec);
./kernel/sched/fair.c:   * 1. 就绪队列本身的cfs_rq->min_vruntime成员
./kernel/sched/fair.c:	if (se != cfs_rq->curr)
./kernel/sched/fair.c:	if (se != cfs_rq->curr)
./kernel/sched/fair.c:	update_load_add(&cfs_rq->load, se->load.weight);
./kernel/sched/fair.c:	cfs_rq->nr_running++;
./kernel/sched/fair.c:	update_load_sub(&cfs_rq->load, se->load.weight);
./kernel/sched/fair.c:	cfs_rq->nr_running--;
./kernel/sched/fair.c:	cfs_rq->runnable_weight += se->runnable_weight;
./kernel/sched/fair.c:	cfs_rq->avg.runnable_load_avg += se->avg.runnable_load_avg;
./kernel/sched/fair.c:	cfs_rq->avg.runnable_load_sum += se_runnable(se) * se->avg.runnable_load_sum;
./kernel/sched/fair.c:	cfs_rq->runnable_weight -= se->runnable_weight;
./kernel/sched/fair.c:	sub_positive(&cfs_rq->avg.runnable_load_avg, se->avg.runnable_load_avg);
./kernel/sched/fair.c:	sub_positive(&cfs_rq->avg.runnable_load_sum,
./kernel/sched/fair.c:	cfs_rq->avg.load_avg += se->avg.load_avg; //系统第一次调用这个函数,cfs_rq->avg.load_avg = 0,se->avg.load_avg=0x400=1024
./kernel/sched/fair.c:	cfs_rq->avg.load_sum += se_weight(se) * se->avg.load_sum;
./kernel/sched/fair.c:	sub_positive(&cfs_rq->avg.load_avg, se->avg.load_avg);
./kernel/sched/fair.c:	sub_positive(&cfs_rq->avg.load_sum, se_weight(se) * se->avg.load_sum);
./kernel/sched/fair.c:		if (cfs_rq->curr == se)
./kernel/sched/fair.c:	struct task_group *tg = cfs_rq->tg;
./kernel/sched/fair.c:	load = max(scale_load_down(cfs_rq->load.weight), cfs_rq->avg.load_avg);
./kernel/sched/fair.c:	tg_weight -= cfs_rq->tg_load_avg_contrib;//在update_load_avg()中可以被更新,然后update_cfs_group()->calc_group_shares()
./kernel/sched/fair.c:	tg_weight += load; //cfs_rq->tg_load_avg_contrib = cfs_rq->avg.load_avg;
./kernel/sched/fair.c:	load_avg = max(cfs_rq->avg.load_avg,
./kernel/sched/fair.c:		       scale_load_down(cfs_rq->load.weight));
./kernel/sched/fair.c:	runnable = max(cfs_rq->avg.runnable_load_avg,
./kernel/sched/fair.c:		       scale_load_down(cfs_rq->runnable_weight));
./kernel/sched/fair.c:	runnable = shares = READ_ONCE(gcfs_rq->tg->shares);
./kernel/sched/fair.c:	long delta = cfs_rq->avg.load_avg - cfs_rq->tg_load_avg_contrib;
./kernel/sched/fair.c:	if (cfs_rq->tg == &root_task_group)
./kernel/sched/fair.c:	if (force || abs(delta) > cfs_rq->tg_load_avg_contrib / 64) {
./kernel/sched/fair.c:		atomic_long_add(delta, &cfs_rq->tg->load_avg);
./kernel/sched/fair.c:		cfs_rq->tg_load_avg_contrib = cfs_rq->avg.load_avg;
./kernel/sched/fair.c:	long delta = gcfs_rq->avg.util_avg - se->avg.util_avg;
./kernel/sched/fair.c:	se->avg.util_avg = gcfs_rq->avg.util_avg;
./kernel/sched/fair.c:	add_positive(&cfs_rq->avg.util_avg, delta);
./kernel/sched/fair.c:	cfs_rq->avg.util_sum = cfs_rq->avg.util_avg * LOAD_AVG_MAX;
./kernel/sched/fair.c:	long delta_avg, running_sum, runnable_sum = gcfs_rq->prop_runnable_sum;
./kernel/sched/fair.c:	gcfs_rq->prop_runnable_sum = 0;
./kernel/sched/fair.c:		if (scale_load_down(gcfs_rq->load.weight)) {
./kernel/sched/fair.c:			load_sum = div_s64(gcfs_rq->avg.load_sum,
./kernel/sched/fair.c:				scale_load_down(gcfs_rq->load.weight));
./kernel/sched/fair.c:	add_positive(&cfs_rq->avg.load_avg, delta_avg);
./kernel/sched/fair.c:	add_positive(&cfs_rq->avg.load_sum, delta_sum);
./kernel/sched/fair.c:		add_positive(&cfs_rq->avg.runnable_load_avg, delta_avg);
./kernel/sched/fair.c:		add_positive(&cfs_rq->avg.runnable_load_sum, delta_sum);
./kernel/sched/fair.c:	cfs_rq->propagate = 1;
./kernel/sched/fair.c:	cfs_rq->prop_runnable_sum += runnable_sum;
./kernel/sched/fair.c:	if (!gcfs_rq->propagate)
./kernel/sched/fair.c:	gcfs_rq->propagate = 0;
./kernel/sched/fair.c:	add_tg_cfs_propagate(cfs_rq, gcfs_rq->prop_runnable_sum);
./kernel/sched/fair.c:	if (gcfs_rq->propagate)
./kernel/sched/fair.c: * cfs_rq->avg is used for task_h_load() and update_cfs_share() for example.
./kernel/sched/fair.c: * Since both these conditions indicate a changed cfs_rq->avg.load we should
./kernel/sched/fair.c:	struct sched_avg *sa = &cfs_rq->avg;
./kernel/sched/fair.c:	if (cfs_rq->removed.nr) {
./kernel/sched/fair.c:		raw_spin_lock(&cfs_rq->removed.lock);
./kernel/sched/fair.c:		swap(cfs_rq->removed.util_avg, removed_util);
./kernel/sched/fair.c:		swap(cfs_rq->removed.load_avg, removed_load);
./kernel/sched/fair.c:		swap(cfs_rq->removed.runnable_sum, removed_runnable_sum);
./kernel/sched/fair.c:		cfs_rq->removed.nr = 0;
./kernel/sched/fair.c:		raw_spin_unlock(&cfs_rq->removed.lock);
./kernel/sched/fair.c:	cfs_rq->load_last_update_time_copy = sa->last_update_time;
./kernel/sched/fair.c: * cfs_rq->avg.last_update_time being current.
./kernel/sched/fair.c:	u32 divider = LOAD_AVG_MAX - 1024 + cfs_rq->avg.period_contrib;
./kernel/sched/fair.c:	se->avg.last_update_time = cfs_rq->avg.last_update_time;
./kernel/sched/fair.c:	se->avg.period_contrib = cfs_rq->avg.period_contrib;
./kernel/sched/fair.c:	cfs_rq->avg.util_avg += se->avg.util_avg;
./kernel/sched/fair.c:	cfs_rq->avg.util_sum += se->avg.util_sum;
./kernel/sched/fair.c: * cfs_rq->avg.last_update_time being current.
./kernel/sched/fair.c:	sub_positive(&cfs_rq->avg.util_avg, se->avg.util_avg);
./kernel/sched/fair.c:	sub_positive(&cfs_rq->avg.util_sum, se->avg.util_sum);
./kernel/sched/fair.c:		last_update_time_copy = cfs_rq->load_last_update_time_copy;
./kernel/sched/fair.c:		last_update_time = cfs_rq->avg.last_update_time;
./kernel/sched/fair.c:	return cfs_rq->avg.last_update_time;
./kernel/sched/fair.c:	raw_spin_lock_irqsave(&cfs_rq->removed.lock, flags);
./kernel/sched/fair.c:	++cfs_rq->removed.nr;
./kernel/sched/fair.c:	cfs_rq->removed.util_avg	+= se->avg.util_avg;
./kernel/sched/fair.c:	cfs_rq->removed.load_avg	+= se->avg.load_avg;
./kernel/sched/fair.c:	cfs_rq->removed.runnable_sum	+= se->avg.load_sum; /* == runnable_sum */
./kernel/sched/fair.c:	raw_spin_unlock_irqrestore(&cfs_rq->removed.lock, flags);
./kernel/sched/fair.c:	return cfs_rq->avg.runnable_load_avg;
./kernel/sched/fair.c:	return cfs_rq->avg.load_avg;
./kernel/sched/fair.c:	enqueued  = cfs_rq->avg.util_est.enqueued;
./kernel/sched/fair.c:	WRITE_ONCE(cfs_rq->avg.util_est.enqueued, enqueued);
./kernel/sched/fair.c:	ue.enqueued  = cfs_rq->avg.util_est.enqueued;
./kernel/sched/fair.c:	WRITE_ONCE(cfs_rq->avg.util_est.enqueued, ue.enqueued);
./kernel/sched/fair.c:	s64 d = se->vruntime - cfs_rq->min_vruntime;
./kernel/sched/fair.c:		schedstat_inc(cfs_rq->nr_spread_over);
./kernel/sched/fair.c:	u64 vruntime = cfs_rq->min_vruntime;
./kernel/sched/fair.c:	bool curr = cfs_rq->curr == se;
./kernel/sched/fair.c:		se->vruntime += cfs_rq->min_vruntime;
./kernel/sched/fair.c:		se->vruntime += cfs_rq->min_vruntime;
./kernel/sched/fair.c:	 *   - Add its load to cfs_rq->runnable_avg
./kernel/sched/fair.c:	 *   - Add its new weight to cfs_rq->load.weight
./kernel/sched/fair.c:	if (cfs_rq->nr_running == 1) {
./kernel/sched/fair.c:		if (cfs_rq->last != se)
./kernel/sched/fair.c:		cfs_rq->last = NULL;
./kernel/sched/fair.c:		if (cfs_rq->next != se)
./kernel/sched/fair.c:		cfs_rq->next = NULL;
./kernel/sched/fair.c:		if (cfs_rq->skip != se)
./kernel/sched/fair.c:		cfs_rq->skip = NULL;
./kernel/sched/fair.c:	if (cfs_rq->last == se)
./kernel/sched/fair.c:	if (cfs_rq->next == se)
./kernel/sched/fair.c:	if (cfs_rq->skip == se)
./kernel/sched/fair.c:	 *   - Subtract its load from the cfs_rq->runnable_avg.
./kernel/sched/fair.c:	 *   - Subtract its previous weight from cfs_rq->load.weight.
./kernel/sched/fair.c:	if (se != cfs_rq->curr)
./kernel/sched/fair.c:		se->vruntime -= cfs_rq->min_vruntime;
./kernel/sched/fair.c:	cfs_rq->curr = se;
./kernel/sched/fair.c:	if (cfs_rq->skip == se) {
./kernel/sched/fair.c:	if (cfs_rq->last && wakeup_preempt_entity(cfs_rq->last, left) < 1)
./kernel/sched/fair.c:		se = cfs_rq->last;
./kernel/sched/fair.c:	if (cfs_rq->next && wakeup_preempt_entity(cfs_rq->next, left) < 1)
./kernel/sched/fair.c:		se = cfs_rq->next;
./kernel/sched/fair.c:	cfs_rq->curr = NULL;
./kernel/sched/fair.c:	if (cfs_rq->nr_running > 1)
./kernel/sched/fair.c:	struct task_group *tg = cfs_rq->tg;
./kernel/sched/fair.c:	min_amount = sched_cfs_bandwidth_slice() - cfs_rq->runtime_remaining;
./kernel/sched/fair.c:	cfs_rq->runtime_remaining += amount;
./kernel/sched/fair.c:	return cfs_rq->runtime_remaining > 0;
./kernel/sched/fair.c:	cfs_rq->runtime_remaining -= delta_exec;
./kernel/sched/fair.c:	if (likely(cfs_rq->runtime_remaining > 0))
./kernel/sched/fair.c:	if (cfs_rq->throttled)
./kernel/sched/fair.c:	if (!assign_cfs_rq_runtime(cfs_rq) && likely(cfs_rq->curr))
./kernel/sched/fair.c:	if (!cfs_bandwidth_used() || !cfs_rq->runtime_enabled)
./kernel/sched/fair.c:	return cfs_bandwidth_used() && cfs_rq->throttled;
./kernel/sched/fair.c:	return cfs_bandwidth_used() && cfs_rq->throttle_count;
./kernel/sched/fair.c:	cfs_rq->throttle_count--;
./kernel/sched/fair.c:	if (!cfs_rq->throttle_count) {
./kernel/sched/fair.c:		cfs_rq->throttled_clock_task_time += rq_clock_task(rq) -
./kernel/sched/fair.c:					     cfs_rq->throttled_clock_task;
./kernel/sched/fair.c:		if (cfs_rq->nr_running >= 1)
./kernel/sched/fair.c:	if (!cfs_rq->throttle_count) {
./kernel/sched/fair.c:		cfs_rq->throttled_clock_task = rq_clock_task(rq);
./kernel/sched/fair.c:	cfs_rq->throttle_count++;
./kernel/sched/fair.c:	struct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);
./kernel/sched/fair.c:	se = cfs_rq->tg->se[cpu_of(rq_of(cfs_rq))];
./kernel/sched/fair.c:  /* task_group可以父子关系嵌套。walk_tg_tree_from()函数功能是顺着cfs_rq->tg往下便利每一个child task_group，并且对每个task_group调用tg_throttle_down()函数。tg_throttle_down()负责增加cfs_rq->throttle_count计数 */
./kernel/sched/fair.c:	walk_tg_tree_from(cfs_rq->tg, tg_throttle_down, tg_nop, (void *)rq);
./kernel/sched/fair.c:	task_delta = cfs_rq->h_nr_running;
./kernel/sched/fair.c:	idle_task_delta = cfs_rq->idle_h_nr_running;
./kernel/sched/fair.c:		qcfs_rq->h_nr_running -= task_delta;
./kernel/sched/fair.c:		qcfs_rq->idle_h_nr_running -= idle_task_delta;
./kernel/sched/fair.c:    /* 如果qcfs_rq运行的进程只有即将被dequeue的se一个的话，那么parent se也需要dequeue。如果qcfs_rq->load.weight不为0，说明qcfs_rq就绪队列上运行的进程不止se一个，那么parent se理所应当不能被dequeue */
./kernel/sched/fair.c:		if (qcfs_rq->load.weight)
./kernel/sched/fair.c:	cfs_rq->throttled = 1;
./kernel/sched/fair.c:	cfs_rq->throttled_clock = rq_clock(rq);
./kernel/sched/fair.c:		list_add_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);
./kernel/sched/fair.c:		list_add_tail_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);
./kernel/sched/fair.c:	struct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);
./kernel/sched/fair.c:	se = cfs_rq->tg->se[cpu_of(rq)];
./kernel/sched/fair.c:	cfs_rq->throttled = 0;
./kernel/sched/fair.c:	cfs_b->throttled_time += rq_clock(rq) - cfs_rq->throttled_clock;
./kernel/sched/fair.c:	list_del_rcu(&cfs_rq->throttled_list);
./kernel/sched/fair.c:	walk_tg_tree_from(cfs_rq->tg, tg_nop, tg_unthrottle_up, (void *)rq);
./kernel/sched/fair.c:	if (!cfs_rq->load.weight)
./kernel/sched/fair.c:	task_delta = cfs_rq->h_nr_running;
./kernel/sched/fair.c:	idle_task_delta = cfs_rq->idle_h_nr_running;
./kernel/sched/fair.c:		cfs_rq->h_nr_running += task_delta;
./kernel/sched/fair.c:		cfs_rq->idle_h_nr_running += idle_task_delta;
./kernel/sched/fair.c:		SCHED_WARN_ON(cfs_rq->runtime_remaining > 0);
./kernel/sched/fair.c:		runtime = -cfs_rq->runtime_remaining + 1;
./kernel/sched/fair.c:		cfs_rq->runtime_remaining += runtime;
./kernel/sched/fair.c:    /* 如果从全局时间池借到的时间保证cfs_rq->runtime_remaining的值应该大于0，执行unthrottle操作 */
./kernel/sched/fair.c:		if (cfs_rq->runtime_remaining > 0)
./kernel/sched/fair.c:	struct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);
./kernel/sched/fair.c:	s64 slack_runtime = cfs_rq->runtime_remaining - min_cfs_rq_runtime;
./kernel/sched/fair.c:	cfs_rq->runtime_remaining -= slack_runtime;
./kernel/sched/fair.c:	if (!cfs_rq->runtime_enabled || cfs_rq->nr_running)
./kernel/sched/fair.c:	if (!cfs_rq->runtime_enabled || cfs_rq->curr)
./kernel/sched/fair.c:	if (cfs_rq->runtime_remaining <= 0)
./kernel/sched/fair.c:	cfs_rq->throttle_count = pcfs_rq->throttle_count;
./kernel/sched/fair.c:	cfs_rq->throttled_clock_task = rq_clock_task(cpu_rq(cpu));
./kernel/sched/fair.c:	if (likely(!cfs_rq->runtime_enabled || cfs_rq->runtime_remaining > 0))
./kernel/sched/fair.c:	cfs_rq->runtime_enabled = 0;
./kernel/sched/fair.c:	INIT_LIST_HEAD(&cfs_rq->throttled_list);
./kernel/sched/fair.c:		cfs_rq->runtime_enabled = cfs_b->quota != RUNTIME_INF;
./kernel/sched/fair.c:		if (!cfs_rq->runtime_enabled)
./kernel/sched/fair.c:		cfs_rq->runtime_remaining = 1;
./kernel/sched/fair.c:		cfs_rq->runtime_enabled = 0;
./kernel/sched/fair.c:		cfs_rq->h_nr_running++;
./kernel/sched/fair.c:		cfs_rq->idle_h_nr_running += idle_h_nr_running;
./kernel/sched/fair.c:		cfs_rq->h_nr_running++;
./kernel/sched/fair.c:		cfs_rq->idle_h_nr_running += idle_h_nr_running;
./kernel/sched/fair.c:		cfs_rq->h_nr_running--;
./kernel/sched/fair.c:		cfs_rq->idle_h_nr_running -= idle_h_nr_running;
./kernel/sched/fair.c:		if (cfs_rq->load.weight) {
./kernel/sched/fair.c:		cfs_rq->h_nr_running--;
./kernel/sched/fair.c:		cfs_rq->idle_h_nr_running -= idle_h_nr_running;
./kernel/sched/fair.c:	util = READ_ONCE(cfs_rq->avg.util_avg);
./kernel/sched/fair.c:		util = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));
./kernel/sched/fair.c:	util = READ_ONCE(cfs_rq->avg.util_avg);
./kernel/sched/fair.c:			READ_ONCE(cfs_rq->avg.util_est.enqueued);
./kernel/sched/fair.c:	unsigned long util_est, util = READ_ONCE(cfs_rq->avg.util_avg);
./kernel/sched/fair.c:		util_est = READ_ONCE(cfs_rq->avg.util_est.enqueued);
./kernel/sched/fair.c:		 * appear in the cfs_rq->avg.util_est.enqueued of any rq,
./kernel/sched/fair.c:			min_vruntime_copy = cfs_rq->min_vruntime_copy;
./kernel/sched/fair.c:			min_vruntime = cfs_rq->min_vruntime;
./kernel/sched/fair.c:		min_vruntime = cfs_rq->min_vruntime;
./kernel/sched/fair.c:	int scale = cfs_rq->nr_running >= sched_nr_latency;
./kernel/sched/fair.c:		struct sched_entity *curr = cfs_rq->curr;
./kernel/sched/fair.c:		 * have to consider cfs_rq->curr. If it is still a runnable
./kernel/sched/fair.c:				if (!cfs_rq->nr_running)
./kernel/sched/fair.c:	if (cfs_rq->avg.load_avg)
./kernel/sched/fair.c:	if (cfs_rq->avg.util_avg)
./kernel/sched/fair.c:	if (cfs_rq->load.weight)
./kernel/sched/fair.c:	if (cfs_rq->avg.load_sum)
./kernel/sched/fair.c:	if (cfs_rq->avg.util_sum)
./kernel/sched/fair.c:	if (cfs_rq->avg.runnable_load_sum)
./kernel/sched/fair.c:		se = cfs_rq->tg->se[cpu];
./kernel/sched/fair.c:	struct sched_entity *se = cfs_rq->tg->se[cpu_of(rq)];
./kernel/sched/fair.c:	if (cfs_rq->last_h_load_update == now)
./kernel/sched/fair.c:	WRITE_ONCE(cfs_rq->h_load_next, NULL);
./kernel/sched/fair.c:		WRITE_ONCE(cfs_rq->h_load_next, se);
./kernel/sched/fair.c:		if (cfs_rq->last_h_load_update == now)
./kernel/sched/fair.c:		cfs_rq->h_load = cfs_rq_load_avg(cfs_rq);
./kernel/sched/fair.c:		cfs_rq->last_h_load_update = now;
./kernel/sched/fair.c:	while ((se = READ_ONCE(cfs_rq->h_load_next)) != NULL) {
./kernel/sched/fair.c:		load = cfs_rq->h_load;
./kernel/sched/fair.c:		cfs_rq->h_load = load;
./kernel/sched/fair.c:		cfs_rq->last_h_load_update = now;
./kernel/sched/fair.c:	return div64_ul(p->se.avg.load_avg * cfs_rq->h_load,
./kernel/sched/fair.c:	curr = cfs_rq->curr;
./kernel/sched/fair.c: 